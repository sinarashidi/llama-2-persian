{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/llama2/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.27s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'llama2-v3'\n",
    "device_map = {\"\": 0}\n",
    "use_4bit = True\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "    # Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\" # Fix weird overflow issue with fp16 training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "test_data = pd.read_csv('3. data_sentiment_without_label.csv')\n",
    "negative = [0 for i in range(len(test_data))]\n",
    "neutral = [0 for i in range(len(test_data))]\n",
    "positive = [0 for i in range(len(test_data))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/llama2/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "for i, d in enumerate(test_data['tweet']):\n",
    "    prompt = f\"\"\"what is the sentiment of the following sentence.\n",
    "    Your respose should be only one word: positive, negative or neutral.\n",
    "    Sentence: {d}\n",
    "\n",
    "    \"\"\"\n",
    "    encode = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    length = len(encode)\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=length+12)\n",
    "\n",
    "    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "    # print(result[0]['generated_text'])\n",
    "    match result[0]['generated_text'].split()[-1]:\n",
    "        case 'منفی':\n",
    "            negative[i] = 1\n",
    "        case 'خنثی':\n",
    "            neutral[i] = 1\n",
    "        case 'مثبت':\n",
    "            positive[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['negative'] = negative\n",
    "test_data['positive'] = positive\n",
    "test_data['neutral'] = neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>local_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>983807604982996995</td>\n",
       "      <td>دوستان بارسایی. \\nحالا بشینید و فوتبال خوب تما...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1058990279196323840</td>\n",
       "      <td>اگر بازی استقلال. جوبیلو ایواتا توی استادیوم آ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1202268201859588097</td>\n",
       "      <td>یکی از کلان پروژه‌های دوران امامت #امام_حسن_عس...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1206882143718268928</td>\n",
       "      <td>جریان #غربگرا در حالی برای پوشش ناکارآمدی‌اش، ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1226924630809726980</td>\n",
       "      <td>زمانی که به‌نام #وحدت ایرادات اساسی یک ساختار ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1434536742573412352</td>\n",
       "      <td>عمیقا معتقدم بهترین گزینه موجود استانداری #خوز...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1435180059929759744</td>\n",
       "      <td>امیدوارم بچه حزب‌الهی‌ها اونقدر در آمریکاستیزی...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1435719814585196547</td>\n",
       "      <td>وقتی با شخص #مناسب هستی\\nمجبور نیستی برای خوشح...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1438216598939213833</td>\n",
       "      <td>حرفهایی امشب #مددی در مورد #استقلال  دقیقا منو...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1440553181788835848</td>\n",
       "      <td>ولی اون آخوندایی که صدر تظاهرات علیه تزریق واک...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               local_id                                              tweet  \\\n",
       "0    983807604982996995  دوستان بارسایی. \\nحالا بشینید و فوتبال خوب تما...   \n",
       "1   1058990279196323840  اگر بازی استقلال. جوبیلو ایواتا توی استادیوم آ...   \n",
       "2   1202268201859588097  یکی از کلان پروژه‌های دوران امامت #امام_حسن_عس...   \n",
       "3   1206882143718268928  جریان #غربگرا در حالی برای پوشش ناکارآمدی‌اش، ...   \n",
       "4   1226924630809726980  زمانی که به‌نام #وحدت ایرادات اساسی یک ساختار ...   \n",
       "..                  ...                                                ...   \n",
       "95  1434536742573412352  عمیقا معتقدم بهترین گزینه موجود استانداری #خوز...   \n",
       "96  1435180059929759744  امیدوارم بچه حزب‌الهی‌ها اونقدر در آمریکاستیزی...   \n",
       "97  1435719814585196547  وقتی با شخص #مناسب هستی\\nمجبور نیستی برای خوشح...   \n",
       "98  1438216598939213833  حرفهایی امشب #مددی در مورد #استقلال  دقیقا منو...   \n",
       "99  1440553181788835848  ولی اون آخوندایی که صدر تظاهرات علیه تزریق واک...   \n",
       "\n",
       "    negative  neutral  positive  \n",
       "0          0        0         1  \n",
       "1          0        0         1  \n",
       "2          0        0         1  \n",
       "3          0        0         1  \n",
       "4          0        0         1  \n",
       "..       ...      ...       ...  \n",
       "95         0        0         1  \n",
       "96         0        0         1  \n",
       "97         0        0         1  \n",
       "98         0        0         1  \n",
       "99         0        1         0  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = ['local_id', 'tweet', 'negative', 'neutral', 'positive']\n",
    "test_data = test_data[new]\n",
    "test_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('test_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]\n",
      "/home/user01/llama2/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] \n",
      "Identify the following items from the review text:\n",
      "- Sentiment (positive or negative)\n",
      "- Is the reviewer expressing anger? (true or false)\n",
      "- Item purchased by reviewer\n",
      "- Company that made the item\n",
      "The review is delimited with triple backticks. Format your response as a JSON object with \"Sentiment\", \"Anger\", \"Item\" and \"Brand\" as the keys.\n",
      "If the information isn't present, use \"unknown\" as the value.\n",
      "Make your response as short as possible.\n",
      "Format the Anger value as a boolean.\n",
      "Review text: '''من کلا از شیائومی راضی هستم؛ قبلا ترازو و دستگاه تصفیه هواشو گرفتم، الان مسواکشو خریداری کردم. پیشنهاد میکنم''' [/INST] جان را در نظر بگیرید، او یک مسواکشو خریداری کرده است. او رایی دارد و قبلا ترازو و دستگاه تصفیه هواشو گرفت. او پیشنهاد می کند. او رایی دارد. او رایی دارد. او رایی دارد. او رایی دارد. او رایی دارد. او رایی دارد. او رایی دارد. او رایی دارد. او رایی دا\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Identify the following items from the review text:\n",
    "- Sentiment (positive or negative)\n",
    "- Is the reviewer expressing anger? (true or false)\n",
    "- Item purchased by reviewer\n",
    "- Company that made the item\n",
    "The review is delimited with triple backticks. \\\n",
    "Format your response as a JSON object with \\\n",
    "\"Sentiment\", \"Anger\", \"Item\" and \"Brand\" as the keys.\n",
    "If the information isn't present, use \"unknown\" \\\n",
    "as the value.\n",
    "Make your response as short as possible.\n",
    "Format the Anger value as a boolean.\n",
    "Review text: '''من کلا از شیائومی راضی هستم؛ قبلا ترازو و دستگاه تصفیه هواشو گرفتم، الان مسواکشو خریداری کردم. پیشنهاد میکنم'''\"\"\"\n",
    "run(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/llama2/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.99s/it]\n",
      "/home/user01/llama2/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] مدل زبانی چیست؟ [/INST]\n"
     ]
    }
   ],
   "source": [
    "from run_model import run\n",
    "\n",
    "model = 'llama2-v2'\n",
    "\n",
    "prompt = 'مدل زبانی چیست؟'\n",
    "run(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
