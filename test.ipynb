{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/llama2/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sinarashidi/llama-2-7b-chat-persian\"\n",
    "# new_model = \"llama-2-7b-chat-persian\"\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 632/632 [00:00<00:00, 6.02MB/s]\n",
      "Downloading (â€¦)model.bin.index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.8k/26.8k [00:00<00:00, 2.62MB/s]\n",
      "Downloading (â€¦)l-00001-of-00002.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.98G/9.98G [23:21<00:00, 7.12MB/s]\n",
      "Downloading (â€¦)l-00002-of-00002.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.50G/3.50G [08:09<00:00, 7.16MB/s]\n",
      "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [31:32<00:00, 946.50s/it] \n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.31s/it]\n",
      "Downloading (â€¦)neration_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174/174 [00:00<00:00, 2.03MB/s]\n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 695/695 [00:00<00:00, 7.40MB/s]\n",
      "Downloading (â€¦)/main/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.84M/1.84M [00:00<00:00, 3.07MB/s]\n",
      "Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 434/434 [00:00<00:00, 4.67MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø§Ø¯Ø±Ø¨Ø±Ø¯ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯ [/INST] Ù…Ø§Ø¯Ø±Ø¨Ø±Ø¯ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± ÛŒÚ© ØªÚ©Ù†ÛŒÚ© Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨Ø§ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¯ÛŒÚ¯Ø± Ù…Ø´Ø§Ø¨Ù‡ Ø§Ø³Øª. Ø§ÛŒÙ† ØªÚ©Ù†ÛŒÚ© Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒ Ø´ÙˆØ¯ Ú©Ù‡ Ø¨Ø§ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¯ÛŒÚ¯Ø± Ù…Ø´Ø§Ø¨Ù‡ Ø§Ø³ØªØŒ Ø§Ù…Ø§ Ø¨Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù ÛŒØ§ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ù‡Ø§ÛŒ Ù…ØªÙØ§ÙˆØª ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒ Ø´ÙˆØ¯. Ø§ÛŒÙ† Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ Ø§ÙØ²Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ù‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ú©Ù…Ú© Ú©Ù†Ø¯ Ùˆ Ø¨Ù‡ Ú©Ø§Ù‡Ø´ Ù‡Ø²ÛŒÙ†Ù‡ Ù‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú©Ù…Ú© Ú©Ù†Ø¯ Ùˆ Ø¨Ù‡ Ú©Ø§Ù‡Ø´ ØªØ¹Ø¯Ø§Ø¯ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ù‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ú©Ù…Ú© Ú©Ù†Ø¯ Ú©Ù‡ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ù‡Ø§ÛŒ Ø§Ù†Ø¨ÙˆÙ‡ Ø´Ù†Ø§Ø®ØªÙ‡ Ù…ÛŒ Ø´ÙˆÙ†Ø¯. Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø§Ø¯Ø±Ø¨Ø±Ø¯ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±ØŒ Ù…ÛŒ ØªÙˆØ§Ù†ÛŒÙ… Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ù‡Ø§ÛŒ Ù…ØªÙØ§ÙˆØª Ø±Ø§ \n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø§Ø¯Ø±Ø¨Ø±Ø¯ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model-00001-of-00002.bin:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 3.96G/9.98G [20:17<55:51, 1.80MB/s]    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mpush_to_hub(new_model, use_temp_dir\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[39m.\u001b[39mpush_to_hub(new_model, use_temp_dir\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:816\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, use_auth_token, max_shard_size, create_pr, safe_serialization, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39m# Save all files.\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_pretrained(work_dir, max_shard_size\u001b[39m=\u001b[39mmax_shard_size, safe_serialization\u001b[39m=\u001b[39msafe_serialization)\n\u001b[0;32m--> 816\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_upload_modified_files(\n\u001b[1;32m    817\u001b[0m     work_dir,\n\u001b[1;32m    818\u001b[0m     repo_id,\n\u001b[1;32m    819\u001b[0m     files_timestamps,\n\u001b[1;32m    820\u001b[0m     commit_message\u001b[39m=\u001b[39;49mcommit_message,\n\u001b[1;32m    821\u001b[0m     token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    822\u001b[0m     create_pr\u001b[39m=\u001b[39;49mcreate_pr,\n\u001b[1;32m    823\u001b[0m )\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:729\u001b[0m, in \u001b[0;36mPushToHubMixin._upload_modified_files\u001b[0;34m(self, working_dir, repo_id, files_timestamps, commit_message, token, create_pr)\u001b[0m\n\u001b[1;32m    724\u001b[0m         operations\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    725\u001b[0m             CommitOperationAdd(path_or_fileobj\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(working_dir, file), path_in_repo\u001b[39m=\u001b[39mfile)\n\u001b[1;32m    726\u001b[0m         )\n\u001b[1;32m    728\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUploading the following files to \u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(modified_files)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m create_commit(\n\u001b[1;32m    730\u001b[0m     repo_id\u001b[39m=\u001b[39;49mrepo_id, operations\u001b[39m=\u001b[39;49moperations, commit_message\u001b[39m=\u001b[39;49mcommit_message, token\u001b[39m=\u001b[39;49mtoken, create_pr\u001b[39m=\u001b[39;49mcreate_pr\n\u001b[1;32m    731\u001b[0m )\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/huggingface_hub/hf_api.py:828\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_as_future(fn, \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    827\u001b[0m \u001b[39m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/huggingface_hub/hf_api.py:2695\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   2686\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[1;32m   2687\u001b[0m files_to_copy \u001b[39m=\u001b[39m fetch_lfs_files_to_copy(\n\u001b[1;32m   2688\u001b[0m     copies\u001b[39m=\u001b[39mcopies,\n\u001b[1;32m   2689\u001b[0m     repo_type\u001b[39m=\u001b[39mrepo_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2693\u001b[0m     endpoint\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint,\n\u001b[1;32m   2694\u001b[0m )\n\u001b[0;32m-> 2695\u001b[0m upload_lfs_files(\n\u001b[1;32m   2696\u001b[0m     additions\u001b[39m=\u001b[39;49m[addition \u001b[39mfor\u001b[39;49;00m addition \u001b[39min\u001b[39;49;00m additions \u001b[39mif\u001b[39;49;00m upload_modes[addition\u001b[39m.\u001b[39;49mpath_in_repo] \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mlfs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   2697\u001b[0m     repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m   2698\u001b[0m     repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[1;32m   2699\u001b[0m     token\u001b[39m=\u001b[39;49mtoken \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoken,\n\u001b[1;32m   2700\u001b[0m     endpoint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendpoint,\n\u001b[1;32m   2701\u001b[0m     num_threads\u001b[39m=\u001b[39;49mnum_threads,\n\u001b[1;32m   2702\u001b[0m )\n\u001b[1;32m   2703\u001b[0m commit_payload \u001b[39m=\u001b[39m prepare_commit_payload(\n\u001b[1;32m   2704\u001b[0m     operations\u001b[39m=\u001b[39moperations,\n\u001b[1;32m   2705\u001b[0m     upload_modes\u001b[39m=\u001b[39mupload_modes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2709\u001b[0m     parent_commit\u001b[39m=\u001b[39mparent_commit,\n\u001b[1;32m   2710\u001b[0m )\n\u001b[1;32m   2711\u001b[0m commit_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint\u001b[39m}\u001b[39;00m\u001b[39m/api/\u001b[39m\u001b[39m{\u001b[39;00mrepo_type\u001b[39m}\u001b[39;00m\u001b[39ms/\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m/commit/\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/huggingface_hub/_commit_api.py:393\u001b[0m, in \u001b[0;36mupload_lfs_files\u001b[0;34m(additions, repo_type, repo_id, token, endpoint, num_threads)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(filtered_actions) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    392\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mUploading 1 LFS file to the Hub\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m     _wrapped_lfs_upload(filtered_actions[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    394\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    396\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUploading \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(filtered_actions)\u001b[39m}\u001b[39;00m\u001b[39m LFS files to the Hub using up to \u001b[39m\u001b[39m{\u001b[39;00mnum_threads\u001b[39m}\u001b[39;00m\u001b[39m threads concurrently\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m     )\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/huggingface_hub/_commit_api.py:383\u001b[0m, in \u001b[0;36mupload_lfs_files.<locals>._wrapped_lfs_upload\u001b[0;34m(batch_action)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     operation \u001b[39m=\u001b[39m oid2addop[batch_action[\u001b[39m\"\u001b[39m\u001b[39moid\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m--> 383\u001b[0m     lfs_upload(operation\u001b[39m=\u001b[39;49moperation, lfs_batch_action\u001b[39m=\u001b[39;49mbatch_action, token\u001b[39m=\u001b[39;49mtoken)\n\u001b[1;32m    384\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    385\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError while uploading \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00moperation\u001b[39m.\u001b[39mpath_in_repo\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to the Hub.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/huggingface_hub/lfs.py:223\u001b[0m, in \u001b[0;36mlfs_upload\u001b[0;34m(operation, lfs_batch_action, token)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m    220\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    221\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMalformed response from LFS batch endpoint: `chunk_size` should be an integer. Got \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mchunk_size\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m         )\n\u001b[0;32m--> 223\u001b[0m     _upload_multi_part(operation\u001b[39m=\u001b[39;49moperation, header\u001b[39m=\u001b[39;49mheader, chunk_size\u001b[39m=\u001b[39;49mchunk_size, upload_url\u001b[39m=\u001b[39;49mupload_action[\u001b[39m\"\u001b[39;49m\u001b[39mhref\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    224\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     _upload_single_part(operation\u001b[39m=\u001b[39moperation, upload_url\u001b[39m=\u001b[39mupload_action[\u001b[39m\"\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/huggingface_hub/lfs.py:319\u001b[0m, in \u001b[0;36m_upload_multi_part\u001b[0;34m(operation, header, chunk_size, upload_url)\u001b[0m\n\u001b[1;32m    310\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    311\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhf_transfer is enabled but does not support uploading from bytes or BinaryIO, falling back to regular\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m upload\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    314\u001b[0m     use_hf_transfer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    316\u001b[0m response_headers \u001b[39m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m     _upload_parts_hf_transfer(operation\u001b[39m=\u001b[39moperation, sorted_parts_urls\u001b[39m=\u001b[39msorted_parts_urls, chunk_size\u001b[39m=\u001b[39mchunk_size)\n\u001b[1;32m    318\u001b[0m     \u001b[39mif\u001b[39;00m use_hf_transfer\n\u001b[0;32m--> 319\u001b[0m     \u001b[39melse\u001b[39;00m _upload_parts_iteratively(operation\u001b[39m=\u001b[39;49moperation, sorted_parts_urls\u001b[39m=\u001b[39;49msorted_parts_urls, chunk_size\u001b[39m=\u001b[39;49mchunk_size)\n\u001b[1;32m    320\u001b[0m )\n\u001b[1;32m    322\u001b[0m \u001b[39m# 3. Send completion request\u001b[39;00m\n\u001b[1;32m    323\u001b[0m completion_res \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mpost(\n\u001b[1;32m    324\u001b[0m     upload_url,\n\u001b[1;32m    325\u001b[0m     json\u001b[39m=\u001b[39m_get_completion_payload(response_headers, operation\u001b[39m.\u001b[39mupload_info\u001b[39m.\u001b[39msha256\u001b[39m.\u001b[39mhex()),\n\u001b[1;32m    326\u001b[0m     headers\u001b[39m=\u001b[39mLFS_HEADERS,\n\u001b[1;32m    327\u001b[0m )\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/huggingface_hub/lfs.py:375\u001b[0m, in \u001b[0;36m_upload_parts_iteratively\u001b[0;34m(operation, sorted_parts_urls, chunk_size)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39mfor\u001b[39;00m part_idx, part_upload_url \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sorted_parts_urls):\n\u001b[1;32m    370\u001b[0m     \u001b[39mwith\u001b[39;00m SliceFileObj(\n\u001b[1;32m    371\u001b[0m         fileobj,\n\u001b[1;32m    372\u001b[0m         seek_from\u001b[39m=\u001b[39mchunk_size \u001b[39m*\u001b[39m part_idx,\n\u001b[1;32m    373\u001b[0m         read_limit\u001b[39m=\u001b[39mchunk_size,\n\u001b[1;32m    374\u001b[0m     ) \u001b[39mas\u001b[39;00m fileobj_slice:\n\u001b[0;32m--> 375\u001b[0m         part_upload_res \u001b[39m=\u001b[39m http_backoff(\u001b[39m\"\u001b[39;49m\u001b[39mPUT\u001b[39;49m\u001b[39m\"\u001b[39;49m, part_upload_url, data\u001b[39m=\u001b[39;49mfileobj_slice)\n\u001b[1;32m    376\u001b[0m         hf_raise_for_status(part_upload_res)\n\u001b[1;32m    377\u001b[0m         headers\u001b[39m.\u001b[39mappend(part_upload_res\u001b[39m.\u001b[39mheaders)\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:258\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    257\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m response \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    260\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:63\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     64\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m     request_id \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:415\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    413\u001b[0m         conn\u001b[39m.\u001b[39mrequest_chunked(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhttplib_request_kw)\n\u001b[1;32m    414\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m         conn\u001b[39m.\u001b[39;49mrequest(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhttplib_request_kw)\n\u001b[1;32m    417\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBrokenPipeError\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n",
      "File \u001b[0;32m~/llama2/.venv/lib/python3.10/site-packages/urllib3/connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (six\u001b[39m.\u001b[39mensure_str(k\u001b[39m.\u001b[39mlower()) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m headers):\n\u001b[1;32m    243\u001b[0m     headers[\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 244\u001b[0m \u001b[39msuper\u001b[39;49m(HTTPConnection, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\u001b[39mself\u001b[39m, method, url, body\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, headers\u001b[39m=\u001b[39m{}, \u001b[39m*\u001b[39m,\n\u001b[1;32m   1281\u001b[0m             encode_chunked\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1282\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1329\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(body, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1326\u001b[0m     \u001b[39m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m     \u001b[39m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1329\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1077\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[39mif\u001b[39;00m encode_chunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http_vsn \u001b[39m==\u001b[39m \u001b[39m11\u001b[39m:\n\u001b[1;32m   1074\u001b[0m         \u001b[39m# chunked encoding\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m         chunk \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(chunk)\u001b[39m:\u001b[39;00m\u001b[39mX\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m chunk \\\n\u001b[1;32m   1076\u001b[0m             \u001b[39m+\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1077\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(chunk)\n\u001b[1;32m   1079\u001b[0m \u001b[39mif\u001b[39;00m encode_chunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http_vsn \u001b[39m==\u001b[39m \u001b[39m11\u001b[39m:\n\u001b[1;32m   1080\u001b[0m     \u001b[39m# end chunked transfer\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:999\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    997\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m\"\u001b[39m\u001b[39mhttp.client.send\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m, data)\n\u001b[1;32m    998\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 999\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msock\u001b[39m.\u001b[39;49msendall(data)\n\u001b[1;32m   1000\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mIterable):\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1237\u001b[0m, in \u001b[0;36mSSLSocket.sendall\u001b[0;34m(self, data, flags)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         amount \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(byte_view)\n\u001b[1;32m   1236\u001b[0m         \u001b[39mwhile\u001b[39;00m count \u001b[39m<\u001b[39m amount:\n\u001b[0;32m-> 1237\u001b[0m             v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(byte_view[count:])\n\u001b[1;32m   1238\u001b[0m             count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m v\n\u001b[1;32m   1239\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1206\u001b[0m, in \u001b[0;36mSSLSocket.send\u001b[0;34m(self, data, flags)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1203\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1204\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to send() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1205\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1206\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mwrite(data)\n\u001b[1;32m   1207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msend(data, flags)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model-00001-of-00002.bin:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 3.96G/9.98G [20:32<55:50, 1.80MB/s]"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/llama2/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is a language model? [/INST]  A language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text to generate language outputs that are coherent and natural-sounding. everybody. Language models are typically trained on vast amounts of text data, such as books, articles, and websites, and are designed to learn the patterns and structures of language.\n",
      "\n",
      "The goal of a language model is to be able to generate text that is similar to the training data, but not necessarily identical to it. This can involve generating text that is coherent and contextually appropriate, as well as producing novel and creative text that goes beyond the training data.\n",
      "\n",
      "There are several types of language models, including:\n",
      "\n",
      "1. Neural network-based language models: These models use deep neural networks to learn the patterns and structures of language. They are typically trained on large datasets of text and can generate text that is coherent and natural-sounding.\n",
      "2. Statistical language models: These models use statistical methods to analyze the patterns and structures of language. They are typically trained on large datasets of text and can generate text that is coherent and contextually appropriate.\n",
      "3. Hybrid language models: These models combine the strengths of neural network-based and statistical language models. They are typically trained on large datasets of text and can generate text that is both coherent and natural-sounding.\n",
      "\n",
      "Language models have many applications, including:\n",
      "\n",
      "1. Language translation: Language models can be used to translate text from one language to another.\n",
      "2. Text summarization: Language models can be used to summarize long pieces of text, such as articles or documents, into shorter summaries.\n",
      "3. Chatbots: Language models can be used to power chatbots and other conversational AI systems.\n",
      "4. Content generation: Language models can be used to generate content, such as articles, blog posts, and social media posts.\n",
      "5. Language understanding: Language models can be used to understand and interpret natural language text, such as sentiment analysis, entity recognition, and question-answering.\n",
      "\n",
      "Some examples of language models include:\n",
      "\n",
      "1. BERT (Bidirectional Encoder Representations from Transformers): A neural network-based language model developed by Google that has achieved state-of-the\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"What is a language model?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['local_id', 'tweet_id', 'data', 'Not_Claim', 'Trait', 'Action',\n",
       "       'Support_Oppose', 'Prediction', 'Quantity', 'Causation_Correlation',\n",
       "       'Rule_Law', 'Quote', 'Other', 'Comparision'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "output = []\n",
    "data = pd.read_csv('./data/2.train_claim_data_text.csv')\n",
    "data.rename(columns={'Not Claim': 'Not_Claim', 'Support/Oppose': 'Support_Oppose', 'Causation/Correlation': 'Causation_Correlation', 'Rule/Law': 'Rule_Law', 'Other Claim': 'Other'}, inplace=True)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data.itertuples():\n",
    "    question = f'Perform claim detection on the following sentence: {d.data}'\n",
    "    answer = ''\n",
    "    if d.Not_Claim == 1:\n",
    "        answer += \"ØºÛŒØ± Ø§Ø¯Ø¹Ø§ØŒ\"\n",
    "    if d.Trait == 1:\n",
    "        answer += \"ØµÙØªØŒ\"\n",
    "    if d.Action == 1:\n",
    "        answer += \"Ø¹Ù…Ù„ØŒ\"\n",
    "    if d.Support_Oppose == 1:\n",
    "        answer += \"Ù…ÙˆØ§ÙÙ‚Øª/Ù…Ø®Ø§Ù„ÙØªØŒ\"\n",
    "    if d.Prediction == 1:\n",
    "        answer += \"Ù¾ÛŒØ´ Ø¨ÛŒÙ†ÛŒØŒ\"\n",
    "    if d.Quantity == 1:\n",
    "        answer += \"Ø¢Ù…Ø§Ø±ØŒ\"\n",
    "    if d.Causation_Correlation == 1:\n",
    "        answer += \"Ø¹Ù„ÛŒØªØŒ\"\n",
    "    if d.Rule_Law == 1:\n",
    "        answer += \"Ù‚ÙˆØ§Ù†ÛŒÙ†ØŒ\"\n",
    "    if d.Quote == 1:\n",
    "        answer += \"Ù†Ù‚Ù„ Ù‚ÙˆÙ„ØŒ\"\n",
    "    if d.Other == 1:\n",
    "        answer += \"ØºÛŒØ±Ù‡ØŒ\"\n",
    "    if d.Comparision == 1:\n",
    "        answer += \"Ù…Ù‚Ø§ÛŒØ³Ù‡\"\n",
    "\n",
    "    output.append({'text': f'### Human: \\n{question} ### Assistant: \\n{answer}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('claim.jsonl', 'w') as f:\n",
    "    for o in output:\n",
    "        json.dump(o, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_text</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>reply_text</th>\n",
       "      <th>Against</th>\n",
       "      <th>Support</th>\n",
       "      <th>Neither</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4471</td>\n",
       "      <td>1540977338040581888</td>\n",
       "      <td>ÙˆØ¸ÛŒÙÙ‡ Ø®ÙˆØ¯ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒÛŒ Ú©Ø±Ø¯Ù† Ø¨Ø®Ø´Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ Ùˆ ÙØ±Ø§Ù…ÛŒÙ† Ø­...</td>\n",
       "      <td>1540977420563513088</td>\n",
       "      <td>Ø¹Ø²Ù„ Ø­Ø³ÛŒÙ† Ø·Ø§Ø¦Ø¨ Ø§Ø² Ø±ÛŒØ§Ø³Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ù¾Ø§Ù‡ Ø¨Ø¹Ù†ÙˆØ§Ù† Ø¨Ù…Ø¨...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5319</td>\n",
       "      <td>1494077275997888512</td>\n",
       "      <td>Ú©Ù…ÛŒØ³ÛŒÙˆÙ† Ø§ØµÙ„ Û¹Û° Ø¨Ø¹Ø¯ Ø§Ø² Ú¯Ø°Ø´Øª Ú†Ù†Ø¯ÛŒÙ† Ù…Ø§Ù‡ Ø§Ø² Ø·Ø±Ø­ Ø´Ú©...</td>\n",
       "      <td>1494085073670447104</td>\n",
       "      <td>Ø§Ø² Ú©Ù…ÛŒØ³ÛŒÙˆÙ† Ø§ØµÙ„90 Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù…ÛŒÚ©Ù†ÛŒÙ… Ø¯Ø±Ù…ÙˆØ±Ø¯ Ø¯Ø±Ù†Ø¸Ø±...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3741</td>\n",
       "      <td>1543639992110653440</td>\n",
       "      <td>... Ø§Ù†Ø´Ø§Ø§Ù„Ù„Ù‡ Ø®Ø¯Ø§ Ø§Ùˆ Ø±Ø§ Ø¨Ù‡ Ø®Ø§Ø·Ø± Ø®Ø¯Ù…Ø§ØªØ´ Ø¨ÛŒØ§Ù…Ø±Ø²Ø¯....</td>\n",
       "      <td>1543654831537127424</td>\n",
       "      <td>Ø§Ù†Ø³Ø§Ù†â€ŒÙ‡Ø§ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§ÛŒ Ø§Ø² Ø±ÙØªØ§Ø±Ù‡Ø§ÛŒ ØµØ­ÛŒØ­ Ùˆ Ù†Ø§ØµØ­ÛŒØ­ Ù‡...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6272</td>\n",
       "      <td>1492293577556336640</td>\n",
       "      <td>Ú¯Ø²Ø§Ø±Ø´ Ø¯Ø± Ù…ÙˆØ±Ø¯ #Ø¹ÛŒØ³ÛŒ_Ø´Ø±ÛŒÙÛŒ Ø±Ø§ Ø¯Ø± Ø³Ø§Ù„ Û¹Ûµ Ù†ÙˆØ´ØªÙ… Ùˆ...</td>\n",
       "      <td>1492374723669413888</td>\n",
       "      <td>Ø§Ù…Ø§ Ø¹ÛŒØ³ÛŒ Ø´Ø±ÛŒÙÛŒ Ú©Ø±Ù…Ø§Ù†Ø´Ø§Ù‡ÛŒ Ù†ÛŒØ³Øª!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1580</td>\n",
       "      <td>1545306680203812864</td>\n",
       "      <td>ØªÚ©Ù…ÛŒÙ„ÛŒ/ ØªØ²Ø±ÛŒÙ‚ Ù…Ø³ØªÙ…Ø± Ø®ÙˆÙ† Ø¨Ù‡ Ù†Ø®Ø³Øªâ€ŒÙˆØ²ÛŒØ± Ø³Ø§Ø¨Ù‚ Ú˜Ø§Ù¾Ù†...</td>\n",
       "      <td>1545333584977879040</td>\n",
       "      <td>Ù†Ø®Ø³Øªâ€ŒÙˆØ²ÛŒØ± Ø³Ø§Ø¨Ù‚ Ú˜Ø§Ù¾Ù† Ø¬Ø§Ù† Ø¨Ø§Ø®Øª\\n\\nğŸ”¹ Ø´Ø¨Ú©Ù‡ Â«Ø§ÙÙ†â€ŒØ§Ù...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id              post_id  \\\n",
       "0  4471  1540977338040581888   \n",
       "1  5319  1494077275997888512   \n",
       "2  3741  1543639992110653440   \n",
       "3  6272  1492293577556336640   \n",
       "4  1580  1545306680203812864   \n",
       "\n",
       "                                           post_text             reply_id  \\\n",
       "0  ÙˆØ¸ÛŒÙÙ‡ Ø®ÙˆØ¯ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒÛŒ Ú©Ø±Ø¯Ù† Ø¨Ø®Ø´Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ Ùˆ ÙØ±Ø§Ù…ÛŒÙ† Ø­...  1540977420563513088   \n",
       "1  Ú©Ù…ÛŒØ³ÛŒÙˆÙ† Ø§ØµÙ„ Û¹Û° Ø¨Ø¹Ø¯ Ø§Ø² Ú¯Ø°Ø´Øª Ú†Ù†Ø¯ÛŒÙ† Ù…Ø§Ù‡ Ø§Ø² Ø·Ø±Ø­ Ø´Ú©...  1494085073670447104   \n",
       "2  ... Ø§Ù†Ø´Ø§Ø§Ù„Ù„Ù‡ Ø®Ø¯Ø§ Ø§Ùˆ Ø±Ø§ Ø¨Ù‡ Ø®Ø§Ø·Ø± Ø®Ø¯Ù…Ø§ØªØ´ Ø¨ÛŒØ§Ù…Ø±Ø²Ø¯....  1543654831537127424   \n",
       "3  Ú¯Ø²Ø§Ø±Ø´ Ø¯Ø± Ù…ÙˆØ±Ø¯ #Ø¹ÛŒØ³ÛŒ_Ø´Ø±ÛŒÙÛŒ Ø±Ø§ Ø¯Ø± Ø³Ø§Ù„ Û¹Ûµ Ù†ÙˆØ´ØªÙ… Ùˆ...  1492374723669413888   \n",
       "4  ØªÚ©Ù…ÛŒÙ„ÛŒ/ ØªØ²Ø±ÛŒÙ‚ Ù…Ø³ØªÙ…Ø± Ø®ÙˆÙ† Ø¨Ù‡ Ù†Ø®Ø³Øªâ€ŒÙˆØ²ÛŒØ± Ø³Ø§Ø¨Ù‚ Ú˜Ø§Ù¾Ù†...  1545333584977879040   \n",
       "\n",
       "                                          reply_text  Against  Support  \\\n",
       "0  Ø¹Ø²Ù„ Ø­Ø³ÛŒÙ† Ø·Ø§Ø¦Ø¨ Ø§Ø² Ø±ÛŒØ§Ø³Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ù¾Ø§Ù‡ Ø¨Ø¹Ù†ÙˆØ§Ù† Ø¨Ù…Ø¨...        0        1   \n",
       "1    Ø§Ø² Ú©Ù…ÛŒØ³ÛŒÙˆÙ† Ø§ØµÙ„90 Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù…ÛŒÚ©Ù†ÛŒÙ… Ø¯Ø±Ù…ÙˆØ±Ø¯ Ø¯Ø±Ù†Ø¸Ø±...        0        1   \n",
       "2  Ø§Ù†Ø³Ø§Ù†â€ŒÙ‡Ø§ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§ÛŒ Ø§Ø² Ø±ÙØªØ§Ø±Ù‡Ø§ÛŒ ØµØ­ÛŒØ­ Ùˆ Ù†Ø§ØµØ­ÛŒØ­ Ù‡...        0        1   \n",
       "3                     Ø§Ù…Ø§ Ø¹ÛŒØ³ÛŒ Ø´Ø±ÛŒÙÛŒ Ú©Ø±Ù…Ø§Ù†Ø´Ø§Ù‡ÛŒ Ù†ÛŒØ³Øª!        0        0   \n",
       "4  Ù†Ø®Ø³Øªâ€ŒÙˆØ²ÛŒØ± Ø³Ø§Ø¨Ù‚ Ú˜Ø§Ù¾Ù† Ø¬Ø§Ù† Ø¨Ø§Ø®Øª\\n\\nğŸ”¹ Ø´Ø¨Ú©Ù‡ Â«Ø§ÙÙ†â€ŒØ§Ù...        0        1   \n",
       "\n",
       "   Neither  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        1  \n",
       "4        0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/2.train_stance_data_text.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for d in data.itertuples():\n",
    "    question = f'Perform stance detection on the following sentence and its reply sentence:\\nSentence: {d.post_text}\\nReply sentence: {d.reply_text}'\n",
    "    answer = ''\n",
    "    if d.Against == 1:\n",
    "        answer += \"Ù…Ø®Ø§Ù„Ù\"\n",
    "    if d.Support == 1:\n",
    "        answer += \"Ù…ÙˆØ§ÙÙ‚\"\n",
    "    if d.Neither == 1:\n",
    "        answer += \"Ù‡ÛŒÚ†Ú©Ø¯Ø§Ù…\"\n",
    "    \n",
    "    output.append({'text': f'### Human: \\n{question} ### Assistant: \\n{answer}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stance.jsonl', 'w') as f:\n",
    "    for o in output:\n",
    "        json.dump(o, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def combine_jsonl_files(input_files, output_file):\n",
    "    combined_data = []\n",
    "\n",
    "    # Read data from input JSONL files\n",
    "    for input_file in input_files:\n",
    "        with open(input_file, 'r') as f:\n",
    "            for line in f:\n",
    "                json_object = json.loads(line.strip())\n",
    "                combined_data.append(json_object)\n",
    "\n",
    "    # Write combined data to a new JSONL file\n",
    "    with open(output_file, 'w') as f:\n",
    "        for json_object in combined_data:\n",
    "            json_line = json.dumps(json_object, ensure_ascii=False)\n",
    "            f.write(json_line + '\\n')\n",
    "\n",
    "# List of input JSONL files to combine\n",
    "input_files = ['stance.jsonl', 'claim.jsonl', 'sentiment.jsonl']\n",
    "\n",
    "# Output JSONL file to write combined data\n",
    "output_file = 'sentiment_claim_stance.jsonl'\n",
    "\n",
    "# Call the function to combine JSONL files\n",
    "combine_jsonl_files(input_files, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
